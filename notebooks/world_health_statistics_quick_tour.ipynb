{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-12T06:25:01.922412Z",
     "start_time": "2025-04-12T06:25:01.907237Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import pdfplumber\n",
    "import re\n",
    "import json\n",
    "import os"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T06:14:47.410521Z",
     "start_time": "2025-04-12T06:14:47.401618Z"
    }
   },
   "cell_type": "code",
   "source": "path_who = r\"C:\\Users\\Dee\\root\\Projects\\dev\\unpatternedAi\\unstructureparser\\docs\\world_health_statistics_2024.pdf\"",
   "id": "48be396802ccb750",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T06:15:59.486888Z",
     "start_time": "2025-04-12T06:15:40.806749Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with pdfplumber.open(path_who) as pdf:\n",
    "    page = pdf.pages[2]\n",
    "    text = page.extract_text()\n",
    "    \n",
    "    for line in text.split('\\n'):\n",
    "        print(line)"
   ],
   "id": "bb6e02837e2209ae",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "World health\n",
      "statistics 2024\n",
      "Monitoring health for the SDGs,\n",
      "Sustainable Development Goals\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T06:16:10.922004Z",
     "start_time": "2025-04-12T06:16:10.898860Z"
    }
   },
   "cell_type": "code",
   "source": "print(text)",
   "id": "a28fc697985fd465",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "World health\n",
      "statistics 2024\n",
      "Monitoring health for the SDGs,\n",
      "Sustainable Development Goals\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T06:21:57.343116Z",
     "start_time": "2025-04-12T06:21:57.320688Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\" Extracts text from a PDF file and returns a list of dicitionaries with page numbers and text\"\"\"\n",
    "    extracted_data = []\n",
    "    \n",
    "    if not os.path.exists(pdf_path):\n",
    "        raise FileNotFoundError(f\"The file '{pdf_path}' does not exist.\")\n",
    "    \n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            page_num = page.page_number\n",
    "            print(f\"... Extracting text information from page {page_num} ...\")\n",
    "            str_text = page.extract_text()\n",
    "            if str_text: # Only store non-empth pages\n",
    "                extracted_data.append({\n",
    "                    \"page\": page_num,\n",
    "                    \"text\": str_text,\n",
    "                    \"source\": os.path.basename(pdf_path)\n",
    "                })\n",
    "    return extracted_data"
   ],
   "id": "9fcdca4c59746eb2",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T06:22:08.698795Z",
     "start_time": "2025-04-12T06:22:08.667696Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def save_extracted_data(data, output_path):\n",
    "    \"\"\"Saves extracted text data to a JSON file.\"\"\"\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, indent=4, ensure_ascii=False)\n",
    "    print(f\"Extracted data saved to {output_path}\")"
   ],
   "id": "c37ab8df6c6a06eb",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T06:22:19.571527Z",
     "start_time": "2025-04-12T06:22:19.536464Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def process_pdf(pdf_path, output_path):\n",
    "    \"\"\"Main function to process the PDF and save extracted text.\"\"\"\n",
    "    try:\n",
    "        extracted_data = extract_text_from_pdf(pdf_path)\n",
    "        save_extracted_data(extracted_data, output_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing PDF: {e}\")"
   ],
   "id": "3ae52b9065bf",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T06:24:14.927400Z",
     "start_time": "2025-04-12T06:24:14.904068Z"
    }
   },
   "cell_type": "code",
   "source": [
    "who_pdf_path = r\"C:\\Users\\Dee\\root\\Projects\\dev\\unpatternedAi\\unstructureparser\\docs\\world_health_statistics_2024.pdf\"\n",
    "output_who_pdf_path = r\"C:\\Users\\Dee\\root\\Projects\\dev\\unpatternedAi\\unstructureparser\\data\\raw\\who_extracted_pdf.json\""
   ],
   "id": "ce65c20dbbf70dd3",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T06:28:46.921199Z",
     "start_time": "2025-04-12T06:25:07.928410Z"
    }
   },
   "cell_type": "code",
   "source": "process_pdf(who_pdf_path, output_who_pdf_path)",
   "id": "bc158c4c54057a83",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Extracting text information from page 1 ...\n",
      "... Extracting text information from page 2 ...\n",
      "... Extracting text information from page 3 ...\n",
      "... Extracting text information from page 4 ...\n",
      "... Extracting text information from page 5 ...\n",
      "... Extracting text information from page 6 ...\n",
      "... Extracting text information from page 7 ...\n",
      "... Extracting text information from page 8 ...\n",
      "... Extracting text information from page 9 ...\n",
      "... Extracting text information from page 10 ...\n",
      "... Extracting text information from page 11 ...\n",
      "... Extracting text information from page 12 ...\n",
      "... Extracting text information from page 13 ...\n",
      "... Extracting text information from page 14 ...\n",
      "... Extracting text information from page 15 ...\n",
      "... Extracting text information from page 16 ...\n",
      "... Extracting text information from page 17 ...\n",
      "... Extracting text information from page 18 ...\n",
      "... Extracting text information from page 19 ...\n",
      "... Extracting text information from page 20 ...\n",
      "... Extracting text information from page 21 ...\n",
      "... Extracting text information from page 22 ...\n",
      "... Extracting text information from page 23 ...\n",
      "... Extracting text information from page 24 ...\n",
      "... Extracting text information from page 25 ...\n",
      "... Extracting text information from page 26 ...\n",
      "... Extracting text information from page 27 ...\n",
      "... Extracting text information from page 28 ...\n",
      "... Extracting text information from page 29 ...\n",
      "... Extracting text information from page 30 ...\n",
      "... Extracting text information from page 31 ...\n",
      "... Extracting text information from page 32 ...\n",
      "... Extracting text information from page 33 ...\n",
      "... Extracting text information from page 34 ...\n",
      "... Extracting text information from page 35 ...\n",
      "... Extracting text information from page 36 ...\n",
      "... Extracting text information from page 37 ...\n",
      "... Extracting text information from page 38 ...\n",
      "... Extracting text information from page 39 ...\n",
      "... Extracting text information from page 40 ...\n",
      "... Extracting text information from page 41 ...\n",
      "... Extracting text information from page 42 ...\n",
      "... Extracting text information from page 43 ...\n",
      "... Extracting text information from page 44 ...\n",
      "... Extracting text information from page 45 ...\n",
      "... Extracting text information from page 46 ...\n",
      "... Extracting text information from page 47 ...\n",
      "... Extracting text information from page 48 ...\n",
      "... Extracting text information from page 49 ...\n",
      "... Extracting text information from page 50 ...\n",
      "... Extracting text information from page 51 ...\n",
      "... Extracting text information from page 52 ...\n",
      "... Extracting text information from page 53 ...\n",
      "... Extracting text information from page 54 ...\n",
      "... Extracting text information from page 55 ...\n",
      "... Extracting text information from page 56 ...\n",
      "... Extracting text information from page 57 ...\n",
      "... Extracting text information from page 58 ...\n",
      "... Extracting text information from page 59 ...\n",
      "... Extracting text information from page 60 ...\n",
      "... Extracting text information from page 61 ...\n",
      "... Extracting text information from page 62 ...\n",
      "... Extracting text information from page 63 ...\n",
      "... Extracting text information from page 64 ...\n",
      "... Extracting text information from page 65 ...\n",
      "... Extracting text information from page 66 ...\n",
      "... Extracting text information from page 67 ...\n",
      "... Extracting text information from page 68 ...\n",
      "... Extracting text information from page 69 ...\n",
      "... Extracting text information from page 70 ...\n",
      "... Extracting text information from page 71 ...\n",
      "... Extracting text information from page 72 ...\n",
      "... Extracting text information from page 73 ...\n",
      "... Extracting text information from page 74 ...\n",
      "... Extracting text information from page 75 ...\n",
      "... Extracting text information from page 76 ...\n",
      "... Extracting text information from page 77 ...\n",
      "... Extracting text information from page 78 ...\n",
      "... Extracting text information from page 79 ...\n",
      "... Extracting text information from page 80 ...\n",
      "... Extracting text information from page 81 ...\n",
      "... Extracting text information from page 82 ...\n",
      "... Extracting text information from page 83 ...\n",
      "... Extracting text information from page 84 ...\n",
      "... Extracting text information from page 85 ...\n",
      "... Extracting text information from page 86 ...\n",
      "... Extracting text information from page 87 ...\n",
      "... Extracting text information from page 88 ...\n",
      "... Extracting text information from page 89 ...\n",
      "... Extracting text information from page 90 ...\n",
      "... Extracting text information from page 91 ...\n",
      "... Extracting text information from page 92 ...\n",
      "... Extracting text information from page 93 ...\n",
      "... Extracting text information from page 94 ...\n",
      "... Extracting text information from page 95 ...\n",
      "... Extracting text information from page 96 ...\n",
      "Extracted data saved to C:\\Users\\Dee\\root\\Projects\\dev\\unpatternedAi\\unstructureparser\\data\\raw\\who_extracted_pdf.json\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T06:31:35.886981Z",
     "start_time": "2025-04-12T06:31:35.801096Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from dotenv import  load_dotenv\n",
    "load_dotenv()"
   ],
   "id": "25b8f8f76fd8a837",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T06:37:56.763340Z",
     "start_time": "2025-04-12T06:37:56.750165Z"
    }
   },
   "cell_type": "code",
   "source": "raw_json = r\"C:\\Users\\Dee\\root\\Projects\\dev\\unpatternedAi\\unstructureparser\\data\\raw\\who_extracted_pdf.json\"",
   "id": "230b8a0aeffb7c3c",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T07:03:25.362778Z",
     "start_time": "2025-04-12T07:03:17.255145Z"
    }
   },
   "cell_type": "code",
   "source": "openai_client = (api_key=os.getenv(\"OPENAI_API_KEY\", \"your-openai-api-key\"))",
   "id": "ad75d2c7ab0da249",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'OpenAI' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mNameError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[19]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m openai_client = \u001B[43mOpenAI\u001B[49m(api_key=os.getenv(\u001B[33m\"\u001B[39m\u001B[33mOPENAI_API_KEY\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33myour-openai-api-key\u001B[39m\u001B[33m\"\u001B[39m))\n",
      "\u001B[31mNameError\u001B[39m: name 'OpenAI' is not defined"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T07:06:25.534571Z",
     "start_time": "2025-04-12T07:06:22.723601Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import json\n",
    "from openai import OpenAI\n",
    "# 1. Modern OpenAI Client Initialization\n",
    "openai_client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\", \"your-openai-api-key\"))"
   ],
   "id": "698ffa77d2f8f0fe",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T07:11:01.124582Z",
     "start_time": "2025-04-12T07:11:01.106150Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 7. Function to generate an answer using OpenAI's updated API call.\n",
    "def generate_answer(query):\n",
    "    \"\"\"\n",
    "    Combine the query and the retrieved context to generate an answer using OpenAI's Chat API.\n",
    "    \"\"\"\n",
    "   #context = \"\\n\\n\".join(retrieved_context)\n",
    "    prompt = (\n",
    "        f\"You are an expert on global health reports. Given the following context extracted from WHO reports:\\n\"\n",
    "        f\"Answer the following question in a detailed and helpful manner:\\n\"\n",
    "        f\"{query}\\n\"\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        # Updated API call using client.chat.completions.create\n",
    "        # Updated API call using client.chat.completions.create\n",
    "        response = openai_client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",  # or \"gpt-4\" if available\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are an assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.3,\n",
    "            max_tokens=200\n",
    "        )\n",
    "        \n",
    "        # Parse and return the message content as JSON.\n",
    "        return json.loads(response.choices[0].message.content)\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating answer: {e}\")\n",
    "        return {\"error\": \"Sorry, I couldn't generate an answer.\"}"
   ],
   "id": "7885152871a8fc3",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "query = \"What are the global trends in maternal health?\"\n",
   "id": "33de9b5f570beef7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T07:11:06.308732Z",
     "start_time": "2025-04-12T07:11:04.120033Z"
    }
   },
   "cell_type": "code",
   "source": "generate_answer(query)",
   "id": "3451b04b64eb8367",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error generating answer: Expecting value: line 1 column 1 (char 0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'error': \"Sorry, I couldn't generate an answer.\"}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T07:28:35.303433Z",
     "start_time": "2025-04-12T07:28:26.853055Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import json\n",
    "from openai import OpenAI\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from chromadb.utils.embedding_functions import OpenAIEmbeddingFunction\n",
    "\n",
    "# 1. Configure your OpenAI API key.\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\") or \"your-openai-api-key\"\n",
    "openai.api_key = openai_api_key\n",
    "\n",
    "# 1. Modern OpenAI Client Initialization\n",
    "openai_client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\", \"your-openai-api-key\"))\n",
    "\n",
    "# 2. Create an embedding function for ChromaDB using OpenAI.\n",
    "embedding_function = OpenAIEmbeddingFunction(api_key=openai_api_key, model_name=\"text-embedding-ada-002\")\n",
    "\n",
    "# 3. Connect (or create) a persistent ChromaDB client.\n",
    "client = chromadb.PersistentClient(path=\"./vector_db\", settings=Settings())\n",
    "collection_name = \"who_publications\"\n",
    "collection = client.get_or_create_collection(\n",
    "    name=collection_name, \n",
    "    embedding_function=embedding_function\n",
    ")\n",
    "\n",
    "# 4. Function to load the extracted JSON file.\n",
    "def load_extracted_json(json_path):\n",
    "    \"\"\"Loads the JSON file containing extracted PDF text data.\"\"\"\n",
    "    if not os.path.exists(json_path):\n",
    "        raise FileNotFoundError(f\"JSON file not found: {json_path}\")\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "# 5. Function to store documents in ChromaDB.\n",
    "def store_documents(documents):\n",
    "    \"\"\"\n",
    "    Given a list of document dicts (with keys 'source', 'page', and 'text'),\n",
    "    store them in ChromaDB, including both the text and embeddings.\n",
    "    \"\"\"\n",
    "    for item in documents:\n",
    "        doc_id = f\"{item['source']}_page_{item['page']}\"\n",
    "        print(f\"-> Loading document: {doc_id}\")\n",
    "        collection.add(\n",
    "            ids=[doc_id],\n",
    "            documents=[item[\"text\"]],  # Ensure actual text is stored!\n",
    "            metadatas=[{\"page\": item[\"page\"], \"source\": item[\"source\"]}],\n",
    "        )\n",
    "        print(f\"Document {doc_id} successfully stored in ChromaDB.\")\n",
    "\n",
    "# 6. Function to retrieve similar documents from ChromaDB.\n",
    "def retrieve_similar_documents(query, top_k=3):\n",
    "    \"\"\"\n",
    "    Retrieve similar documents based on the query from ChromaDB.\n",
    "    \"\"\"\n",
    "    results = collection.query(query_texts=[query], n_results=top_k)\n",
    "    \n",
    "    # Verify that we have valid document text in the results.\n",
    "    if not results or \"documents\" not in results or not results[\"documents\"]:\n",
    "        return []\n",
    "    \n",
    "    matched_texts = results[\"documents\"][0] if results[\"documents\"][0] else []\n",
    "    return [str(doc) for doc in matched_texts]  # Ensure all docs are strings\n",
    "\n",
    "# 7. Function to generate an answer using OpenAI's updated API call.\n",
    "def generate_answer(query, retrieved_context):\n",
    "    \"\"\"\n",
    "    Combine the query and the retrieved context to generate an answer using OpenAI's Chat API.\n",
    "    Focus on a Monitoring, Learning, and Evaluation (MLE) perspective.\n",
    "    The response should explain if it remains a global challenge,\n",
    "    highlighting mortality rates, economic implications, and persistent challenges.\n",
    "    \"\"\"\n",
    "    context = \"\\n\\n\".join(retrieved_context)\n",
    "    prompt = (\n",
    "        \"You are an expert in global health, impact, and evaluation, specializing in Monitoring, \"\n",
    "        \"Learning, and Evaluation (MLE). Given the following context extracted from WHO reports:\\n\"\n",
    "        f\"{context}\\n\\n\"\n",
    "        \"Answer the following question in a detailed and analytical manner. \"\n",
    "        \"Add number and statistics from WHO publication documents\"\n",
    "        \"Your answer should assess whether the issue remains a critical global problem, \"\n",
    "        \"focusing on mortality rates, economic costs, demographic, and ongoing challenges related to that health issue. \"\n",
    "        f\"Question: {query}\\n\"\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        response = openai_client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",  # or \"gpt-4\" if available\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are an assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.3,\n",
    "            max_tokens=500\n",
    "        )\n",
    "        # Return the plain text answer.\n",
    "        return response.choices[0].message.content.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating answer: {e}\")\n",
    "        return {\"error\": \"Sorry, I couldn't generate an answer.\"}\n",
    "\n",
    "# 8. Main workflow.\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # Define the JSON file path (adjust as needed).\n",
    "        # json_path = os.path.join(os.getcwd(), \"data\", \"raw\", \"who_extracted_pdf.json\")\n",
    "        # \n",
    "        # # Load the extracted data from JSON.\n",
    "        # parsed_json = load_extracted_json(json_path)\n",
    "        # print(f\"Loaded {len(parsed_json)} documents from JSON.\")\n",
    "        \n",
    "        # (Optional) Delete existing records from ChromaDB before re-loading.\n",
    "        # collection.delete()  # Uncomment if you wish to remove existing documents.\n",
    "        # \n",
    "        # # Store documents into ChromaDB.\n",
    "        # store_documents(parsed_json)\n",
    "        \n",
    "        # Use a sample query.\n",
    "        query = \"What are the global trends in maternal health?\"\n",
    "        retrieved_docs = retrieve_similar_documents(query, top_k=5)\n",
    "        \n",
    "        # Do not display retrieved context to the user; use it directly to generate the answer.\n",
    "        final_answer = generate_answer(query, retrieved_docs)\n",
    "        print(\"\\nFinal Answer Generated by OpenAI:\\n\")\n",
    "        print(final_answer)\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n"
   ],
   "id": "b637468ee59b5980",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Answer Generated by OpenAI:\n",
      "\n",
      "Maternal health remains a critical global issue despite some progress made in reducing maternal mortality rates (MMR) over the years. According to WHO reports, the global MMR dropped significantly during the Millennium Development Goals (MDG) era, from 339 deaths per 100,000 live births in 2000 to 227 deaths per 100,000 live births in 2015. However, progress has stagnated since then, with the global MMR only slightly decreasing to 223 deaths per 100,000 live births in 2020.\n",
      "\n",
      "The global MMR reduction has not been consistent across all regions. The African Region has consistently had the highest MMR, despite a 2% average annual rate of reduction. On the other hand, the South-East Asia Region experienced the steepest decline in MMR. This disparity in progress highlights the ongoing challenges and disparities in maternal health outcomes globally.\n",
      "\n",
      "To achieve the Sustainable Development Goals (SDG) global target of an MMR below 70 deaths per 100,000 live births by 2030, significant efforts are required. An average annual rate of reduction of 11.6% is needed between 2021 and 2030, equivalent to averting over 1 million deaths compared to a scenario where stagnation continues.\n",
      "\n",
      "Maternal mortality not only represents a significant loss of life but also has economic costs and demographic implications. The estimated 287,000 maternal deaths globally in 2020 are a stark reminder of the human toll of inadequate maternal health care. Additionally, the economic costs associated with maternal mortality, including healthcare expenses, lost productivity, and the impact on families and communities, are substantial.\n",
      "\n",
      "Challenges related to maternal health include access to quality maternal health services, addressing sociocultural factors that impact women's health, ensuring timely and appropriate care during pregnancy and childbirth, and reducing disparities in health outcomes among different populations. Violence against women, including intimate partner violence, further exacerbates maternal health risks and underscores the need for comprehensive approaches to address women's health holistically.\n",
      "\n",
      "In conclusion, while there have been improvements in maternal health outcomes globally, maternal mortality remains a critical issue with ongoing challenges. Addressing maternal health requires a multi-faceted approach that includes strengthening health systems, improving access to quality care, addressing social determinants of health, and promoting gender equality. Efforts to reduce maternal mortality rates must be prioritized to ensure the well-being of women and contribute to achieving global health goals.\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T06:44:25.925794Z",
     "start_time": "2025-04-12T06:44:25.488779Z"
    }
   },
   "cell_type": "code",
   "source": [
    "query = \"What are the global trends in maternal health?\"\n",
    "retrieved_docs = retrieve_similar_documents(query, top_k=3)\n",
    "\n",
    "# Display retrieved context.\n",
    "display_retrieved_context(query, retrieved_docs)\n",
    "\n",
    "# Generate a final answer using OpenAI based on retrieved documents.\n",
    "final_answer = generate_answer(query, retrieved_docs)\n",
    "print(\"\\nFinal Answer Generated by OpenAI:\\n\")\n",
    "print(final_answer)"
   ],
   "id": "422807aead4f6c47",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 3 Document Excerpts for Query: 'What are the global trends in maternal health?':\n",
      "\n",
      "1. 4\n",
      "Key issues and trends\n",
      "in global health\n",
      "The global population and economic growth, the\n",
      "advancement of science and technology, and the ever-\n",
      "increasing interconnectedness of the world have brought\n",
      "about enormous societal changes along with unprecedented\n",
      "progress, but also multiple and complex challe...\n",
      "\n",
      "2. Figure 2.11 Percentage change in adolescent birth rate, by age group, and the proportion of births among\n",
      "adolescent girls out of total births, globally and by WHO region, 2015â€“2023\n",
      "Eastern Region of the South-East Asia Western Pacific\n",
      "Mediterranean Americas African Region Region Global Region Europe...\n",
      "\n",
      "3. 3.\n",
      "Progress towards WHO Triple Billion targets 55\n",
      "3.1 Healthier populations billion 56\n",
      "3.2 UHC billion 58\n",
      "3.3 Health emergencies protection billion 60\n",
      "3.4 Health-related SDGs and health information system 61\n",
      "3.5 Conclusion 64\n",
      "4.\n",
      "Key issues and trends in global health 65\n",
      "4.1 The double burden of maln...\n",
      "\n",
      "Error generating answer: \n",
      "\n",
      "You tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n",
      "\n",
      "You can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n",
      "\n",
      "Alternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n",
      "\n",
      "A detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
      "\n",
      "\n",
      "Final Answer Generated by OpenAI:\n",
      "\n",
      "{'error': \"Sorry, I couldn't generate an answer.\"}\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "import json\n",
    "import openai\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from chromadb.utils.embedding_functions import OpenAIEmbeddingFunction\n",
    "\n",
    "# 1. Configure your OpenAI API key.\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\") or \"your-openai-api-key\"\n",
    "openai.api_key = openai_api_key\n",
    "\n",
    "# 2. Create an embedding function for ChromaDB using OpenAI.\n",
    "embedding_function = OpenAIEmbeddingFunction(api_key=openai_api_key, model_name=\"text-embedding-ada-002\")\n",
    "\n",
    "# 3. Connect (or create) a persistent ChromaDB client.\n",
    "client = chromadb.PersistentClient(path=\"./vector_db\", settings=Settings())\n",
    "collection_name = \"who_publications\"\n",
    "collection = client.get_or_create_collection(\n",
    "    name=collection_name, \n",
    "    embedding_function=embedding_function\n",
    ")\n",
    "\n",
    "# 4. Function to load (or re-load) documents into ChromaDB.\n",
    "def store_documents(parsed_json):\n",
    "    \"\"\"\n",
    "    Given a list of document dicts (with keys 'source', 'page', and 'text'),\n",
    "    store them in ChromaDB, including both text and embeddings.\n",
    "    \"\"\"\n",
    "    for item in parsed_json:\n",
    "        doc_id = f\"{item['source']}_page_{item['page']}\"\n",
    "        print(f\"-> Loading document: {doc_id}\")\n",
    "        # When using OpenAI embedding function with ChromaDB, you don't need to pre-compute embeddings.\n",
    "        collection.add(\n",
    "            ids=[doc_id],\n",
    "            documents=[item[\"text\"]],  # Ensure the actual text is stored.\n",
    "            metadatas=[{\"page\": item[\"page\"], \"source\": item[\"source\"]}],\n",
    "        )\n",
    "        print(f\"Document {doc_id} successfully stored in ChromaDB.\")\n",
    "\n",
    "# 5. Function to retrieve similar documents from ChromaDB.\n",
    "def retrieve_similar_documents(query, top_k=3):\n",
    "    \"\"\"\n",
    "    Retrieve similar documents based on the query from ChromaDB.\n",
    "    \"\"\"\n",
    "    results = collection.query(query_texts=[query], n_results=top_k)\n",
    "    \n",
    "    # Verify that we have valid document text in the results.\n",
    "    if not results or \"documents\" not in results or not results[\"documents\"]:\n",
    "        return []\n",
    "    \n",
    "    matched_texts = results[\"documents\"][0] if results[\"documents\"][0] else []\n",
    "    return [str(doc) for doc in matched_texts]  # Convert to strings if needed\n",
    "\n",
    "# 6. Function to generate an answer using OpenAI's updated API call.\n",
    "def generate_answer(query, retrieved_context):\n",
    "    \"\"\"\n",
    "    Combine the query and the retrieved context to generate an answer using OpenAI's Chat API.\n",
    "    \"\"\"\n",
    "    context = \"\\n\\n\".join(retrieved_context)\n",
    "    prompt = (\n",
    "        f\"You are an expert on global health reports. Given the following context extracted from WHO reports:\\n\"\n",
    "        f\"{context}\\n\\n\"\n",
    "        f\"Answer the following question in a detailed and helpful manner:\\n\"\n",
    "        f\"{query}\\n\"\n",
    "    )\n",
    "    #     response = openai_client.chat.completions.create(\n",
    "    #     model=\"gpt-4-turbo-preview\",  # Use latest model\n",
    "    #     response_format={\"type\": \"json_object\"},  # Enforce JSON output\n",
    "    #     messages=[{\n",
    "    #         \"role\": \"system\",\n",
    "    #         \"content\": \"\"\"You are a WHO health analyst. Answer questions using provided context. \n",
    "    #                     Include citations using [Source: filename, Page X]. Format response as JSON with \n",
    "    #                     'answer', 'confidence' (0-1), and 'citations' fields.\"\"\"\n",
    "    #     }, {\n",
    "    #         \"role\": \"user\",\n",
    "    #         \"content\": f\"Context:\\n{context_str}\\n\\nQuestion: {query}\"\n",
    "    #     }],\n",
    "    #     temperature=0.2,\n",
    "    #     max_tokens=500\n",
    "    # )\n",
    "    # \n",
    "    try:\n",
    "        # Updated API call using client.chat.completions.create\n",
    "        response = openai_client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",  # or \"gpt-4\" if available\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are an assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.3,\n",
    "            max_tokens=200\n",
    "        )\n",
    "        \n",
    "    \n",
    "        # Parse and return the message content as JSON.\n",
    "        return json.loads(response.choices[0].message.content)\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating answer: {e}\")\n",
    "        return {\"error\": \"Sorry, I couldn't generate an answer.\"}\n",
    "\n",
    "# 7. Optionally, a helper function to display the retrieved context.\n",
    "def display_retrieved_context(query, retrieved_docs):\n",
    "    if not retrieved_docs:\n",
    "        print(f\"\\nNo matching results found for query: '{query}'\\n\")\n",
    "    else:\n",
    "        print(f\"\\nTop {len(retrieved_docs)} Document Excerpts for Query: '{query}':\\n\")\n",
    "        for idx, doc in enumerate(retrieved_docs):\n",
    "            print(f\"{idx+1}. {doc[:300]}...\\n\")  # Show first 300 characters per document\n",
    "\n",
    "# 8. Example workflow.\n",
    "if __name__ == \"__main__\":\n",
    "    # If you haven't already loaded your documents:\n",
    "    # parsed_json should be a list of dictionaries with keys: 'source', 'page', 'text'.\n",
    "    # For example:\n",
    "    # parsed_json = [\n",
    "    #    {\"source\": \"world_health_statistics_2024.pdf\", \"page\": 1, \"text\": \"Your document text here...\"},\n",
    "    #    {\"source\": \"world_health_statistics_2024.pdf\", \"page\": 2, \"text\": \"More text...\"},\n",
    "    #    ... \n",
    "    # ]\n",
    "    # Uncomment below to store documents:\n",
    "    # store_documents(parsed_json)\n",
    "\n",
    "    # Use a sample query.\n",
    "    query = \"What are the global trends in maternal health?\"\n",
    "    retrieved_docs = retrieve_similar_documents(query, top_k=3)\n",
    "    \n",
    "    # Display the retrieved context (optional).\n",
    "    display_retrieved_context(query, retrieved_docs)\n",
    "    \n",
    "    # Generate a final answer using OpenAI based on retrieved documents.\n",
    "    final_answer = generate_answer(query, retrieved_docs)\n",
    "    print(\"\\nFinal Answer Generated by OpenAI:\\n\")\n",
    "    print(final_answer)\n"
   ],
   "id": "dd2db69cb9af09fd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "import json\n",
    "from openai import OpenAI\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from chromadb.utils.embedding_functions import OpenAIEmbeddingFunction\n",
    "from tenacity import retry, stop_after_attempt, wait_exponential\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# 1. Modern OpenAI Client Initialization\n",
    "openai_client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\", \"your-openai-api-key\"))\n",
    "\n",
    "# 2. Enhanced ChromaDB Configuration\n",
    "class VectorDBManager:\n",
    "    def __init__(self, path: str = \"./vector_db\"):\n",
    "        self.client = chromadb.PersistentClient(\n",
    "            path=path,\n",
    "            settings=Settings(allow_reset=True, anonymized_telemetry=False)\n",
    "        )\n",
    "        self.embedding_function = OpenAIEmbeddingFunction(\n",
    "            api_key=openai_client.api_key,\n",
    "            model_name=\"text-embedding-3-small\"  # Updated to latest embedding model\n",
    "        )\n",
    "    \n",
    "    def get_collection(self, name: str):\n",
    "        return self.client.get_or_create_collection(\n",
    "            name=name,\n",
    "            embedding_function=self.embedding_function,\n",
    "            metadata={\"hnsw:space\": \"cosine\"}  # Optimized similarity metric\n",
    "        )\n",
    "\n",
    "vector_db = VectorDBManager()\n",
    "collection = vector_db.get_collection(\"who_publications_v2\")\n",
    "\n",
    "# 3. Batch Document Processing with Metadata\n",
    "def store_documents(documents: List[Dict[str, Any]], batch_size: int = 100):\n",
    "    \"\"\"Batch process documents with improved metadata handling\"\"\"\n",
    "    for i in range(0, len(documents), batch_size):\n",
    "        batch = documents[i:i+batch_size]\n",
    "        ids = [f\"{doc['source']}_pg_{doc['page']}_{i}\" for i, doc in enumerate(batch)]\n",
    "        texts = [doc[\"text\"] for doc in batch]\n",
    "        metadatas = [{\n",
    "            \"source\": doc[\"source\"],\n",
    "            \"page\": doc[\"page\"],\n",
    "            \"doc_type\": \"WHO_report\",  # Add structured metadata\n",
    "            \"timestamp\": doc.get(\"timestamp\", \"2023-01-01\")  # Example metadata field\n",
    "        } for doc in batch]\n",
    "        \n",
    "        collection.upsert(\n",
    "            ids=ids,\n",
    "            documents=texts,\n",
    "            metadatas=metadatas\n",
    "        )\n",
    "        print(f\"Processed batch {i//batch_size + 1}/{(len(documents)-1)//batch_size + 1}\")\n",
    "\n",
    "# 4. Enhanced Retrieval with Hybrid Search\n",
    "@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\n",
    "def retrieve_context(query: str, top_k: int = 5, filter: Dict = None) -> List[Dict]:\n",
    "    \"\"\"Hybrid search with metadata filtering\"\"\"\n",
    "    results = collection.query(\n",
    "        query_texts=[query],\n",
    "        n_results=top_k,\n",
    "        where=filter,  # Metadata filtering\n",
    "        include=[\"documents\", \"metadatas\", \"distances\"]\n",
    "    )\n",
    "    \n",
    "    return [{\n",
    "        \"text\": doc,\n",
    "        \"metadata\": meta,\n",
    "        \"score\": score\n",
    "    } for doc, meta, score in zip(\n",
    "        results[\"documents\"][0],\n",
    "        results[\"metadatas\"][0],\n",
    "        results[\"distances\"][0]\n",
    "    )]\n",
    "\n",
    "# 5. Modern Chat Completion with Structured Output\n",
    "@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\n",
    "def generate_answer(query: str, context: List[Dict]) -> Dict[str, Any]:\n",
    "    \"\"\"Generate answer with citations using latest API\"\"\"\n",
    "    context_str = \"\\n\\n\".join(\n",
    "        f\"Source: {c['metadata']['source']} (Page {c['metadata']['page']}):\\n{c['text']}\"\n",
    "        for c in context\n",
    "    )\n",
    "    \n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=\"gpt-4-turbo-preview\",  # Use latest model\n",
    "        response_format={\"type\": \"json_object\"},  # Enforce JSON output\n",
    "        messages=[{\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"\"\"You are a WHO health analyst. Answer questions using provided context. \n",
    "                        Include citations using [Source: filename, Page X]. Format response as JSON with \n",
    "                        'answer', 'confidence' (0-1), and 'citations' fields.\"\"\"\n",
    "        }, {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Context:\\n{context_str}\\n\\nQuestion: {query}\"\n",
    "        }],\n",
    "        temperature=0.2,\n",
    "        max_tokens=500\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        return json.loads(response.choices[0].message.content)\n",
    "    except json.JSONDecodeError:\n",
    "        return {\"error\": \"Failed to parse model response\"}\n",
    "\n",
    "# 6. Validation and Error Handling\n",
    "def validate_document_structure(docs: List[Dict]) -> bool:\n",
    "    required_keys = {\"source\", \"page\", \"text\"}\n",
    "    return all(required_keys.issubset(doc.keys()) for doc in docs)\n",
    "\n",
    "# 7. Example Workflow with Improvements\n",
    "if __name__ == \"__main__\":\n",
    "    # Sample document validation and ingestion\n",
    "    sample_docs = [\n",
    "        {\"source\": \"world_health_2024.pdf\", \"page\": 1, \"text\": \"...\"},\n",
    "        # ... other documents\n",
    "    ]\n",
    "    \n",
    "    if validate_document_structure(sample_docs):\n",
    "        store_documents(sample_docs, batch_size=50)\n",
    "    else:\n",
    "        print(\"Document structure validation failed\")\n",
    "        exit(1)\n",
    "    \n",
    "    # Query with metadata filtering\n",
    "    query = \"What are the latest maternal mortality rates in Africa?\"\n",
    "    context = retrieve_context(\n",
    "        query,\n",
    "        top_k=5,\n",
    "        filter={\"doc_type\": \"WHO_report\", \"timestamp\": {\"$gte\": \"2022-01-01\"}}\n",
    "    )\n",
    "    \n",
    "    # Generate and display answer\n",
    "    if context:\n",
    "        answer = generate_answer(query, context)\n",
    "        print(\"\\nExpert Analysis:\")\n",
    "        print(answer.get(\"answer\", \"No answer generated\"))\n",
    "        print(\"\\nCitations:\")\n",
    "        for cite in answer.get(\"citations\", []):\n",
    "            print(f\"- {cite}\")\n",
    "    else:\n",
    "        print(\"No relevant documents found\")"
   ],
   "id": "ca812b0ccc7e5e26"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
